---
title: "5. Dimensionality reduction techniques"
output: html_document
---

# 5. Dimensionality reduction techniques

This chapter is about dimensionality reduction techniques.  Principal Component Analysis (PCA) will be applied.

The R script file that created the data file can be viewed [here](https://github.com/MikkoPatronen/IODS-project/blob/master/data/create_human.R). Let us introduce the data: 

## 5.1 Summary of the variables in the data:

```{r}
rm(list=ls()) 
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(MASS)
library(corrplot)
human <- read.table("human.txt")
str(human)
```

The data includes 155 observations of 8 variables. They are:

* **Edu2.FM** : the ratio of female and male populations with secondary education in each country
* **Labo.FM** : the ratio of labour force participation of females and males in each country
* **Life.Exp** : life expectancy at birth
* **Edu.Exp** : Expected years of schooling (years)
* **GNI** : The gross national income
* **Mat.Mor** :The maternal mortality ratio
* **Ado.Birth** : the adolescent birth rate
* **Parli.F** : the share of parliamentary seats held by women


## Graphical overview of the data

```{r}
summary(human)
ggpairs(human)
```
Since the variables are not scaled, the means are quite different across variables.

The ggpairs()-plot shows distributions of the variables and shows the correlations between the variables. The first half of the variables (Edu2.FM, Labo.FM, Edu.Exp and Life.Exp) have negative skewness and the latter half is positively skewed. 

The corrplot() visualizes correlations between variables:

```{r}
cor(human)

res1 <- cor.mtest(human, conf.level = .95)

cor(human) %>% corrplot(p.mat = res1$p, method = "color", type = "upper",
sig.level = c(.001, .01, .05), pch.cex = .9,
insig = "label_sig", pch.col = "white", order = "AOE")
```

Blue color means positive correlation and red negative correlation. Lighter colors mean weaker association and darker colors stronger.

## 5.2. Principal Component Analysis (PCA) on the non-standardized dataset

Next we will conduct a principal component analysis (PCA) on the not standardized human data and show the variability captured by the principal components. A biplot will be drawn to display the observations by the first two principal components.

```{r}
# create and print out a summary of pca_human
pca_human <- prcomp(human)
s <- summary(pca_human)
s

# rounded percetanges of variance captured by each PC
pca_pr <- 100*round(1*s$importance[2, ], digits = 3)

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```

## 5.3. Same with standardized data

Let us standardize the data and then perform the PCA again:

```{r}
# standardize the variables
human_std <- scale(human)

# print out summaries of the standardized variables
summary(human_std)

# create and print out a summary of pca_human
pca_human <- prcomp(human_std)

s <- summary(pca_human)
s

# rounded percetanges of variance captured by each PC
pca_pr <- 100*round(1*s$importance[2, ], digits = 3)

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```

Standardizing the data makes a big difference. The results from un-standardized and standardized datas are very different: the results from un-standardized variables showed that PC1 captures all the variation, but with standardized variables PC1 now captures 53,6% (PC2 captures 16,2% and PC3 9,6%).

The variable Parli.F (the share of parliamentary seats held by women) has strong positive correlation with the variable Labo.FM (the ratio of labour force participation of females and males in each country).
The variable Ado.Birth (the adolescent birth rate) is positively correlated with PC1.

## 5.4 Personal interpretations

PCA reduces dimensions of a dataset into components that explain the variance in the data. The arrows of the variables can be interpreted so that the PC1 consists of Mat.Mor, Ado.Birth, Edu.Exp, Edu2.FM, Life.Exp and GNI.  PC2 consists of Parli.F and Labo.FM. PC1 explains 53,6% of the variance in the data. PC2 explains 16,2% of the variance.

## 5.5 Multiple Correspondence Analysis

The structure and the dimensions of the data:

I picked the variables "breakfast", "evening", "sex", "sugar", "where", "lunch" to form my subdata. Here is some information about the structure of the data and some visualizations:

```{r}
library(FactoMineR)
library(tidyr)
library(dplyr)

data("tea")

tea_mca = tea[, c("breakfast", "evening", "sex", "sugar", "where", "lunch")]

summary(tea_mca)
str(tea_mca)

gather(tea_mca) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") +geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

The dataset tea_mca consists of 300 observations of 6 variables.

## MCA:

```{r}
mca <- MCA(tea_mca, graph = FALSE)

summary(mca)
```

The summary of the MCA lists the explained variances. The first dimension explains 18,55% of the variance the second dimension explains 16,99%, the third 15,29%.

```{r}
plot(mca, invisible=c("ind"), habillage="quali")
```

